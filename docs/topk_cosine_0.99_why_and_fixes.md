# Top-K の cosine 類似度が 0.99 台に張り付く理由と改善策

起きていることはかなり「自然」で、いくつかの要因が重なって **cosine 類似度が 0.99 台に張り付きやすい**状態になっています。以下に、原因の考察と改善策を段階的にまとめます（本リポジトリの `artifacts/song_vectors.parquet` を簡易分析した観察も踏まえた説明です）。

---

## 1. 状況整理（観察）

`artifacts/song_vectors.parquet`（曲数 1220）で、曲ベクトル同士の cosine を概観すると：

- ランダムな曲ペアの cosine：中央値が高め（0.98台）になりやすい
- ある曲に対する Top1（最も近い他曲）の cosine：0.99台に張り付きやすい
- 28列の平均値を見ると `neutral` が突出しやすい
- 列ごとのばらつき（分散/標準偏差）が小さい列があり、ベクトルの向きが似やすい

直感的には「どの曲も似た方向を向いている」= cosine が高くなりやすい条件が揃っています。

---

## 2. 原因の考察（なぜ 0.99 が多発するのか）

### 原因A：全成分が正（非負）で、共通の“ベース成分”が大きい

GoEmotions の確率（っぽい値）は基本的に **0以上**です。非負ベクトル同士は幾何的に同じ象限に集まり、角度が鋭角になりやすく、cosine は高めに出がちです。

さらに `neutral` のように、どの曲にも共通して大きく入る成分があると、みんな同じ方向に引っ張られて **cosine が 1 に近づきます**。

直感的には以下の形です：

- 各曲ベクトル = 共通ベース `b`（例：neutral が大） + 曲ごとの差分 `δ`
- `b` が `δ` より大きいと、cosine は `b` のせいで高くなり、差分が埋もれる

### 原因B：mean pooling（行平均）が“曲の個性”を薄める

歌詞の1行単位の確率を曲単位で平均すると：

- 行数が多い曲ほど平均が安定し、**データセット全体の平均（典型的な感情分布）に近づく**
- 結果として曲間の差がさらに小さくなり、cosine が高止まりしやすい

統計的には「平均化で分散が縮む」方向です（特に元々似ている分布ならなおさらです）。

### 原因C：GoEmotions の出力は「softmax で合計 1」の分布ではない（多ラベル）

GoEmotions は一般に多ラベルで、各ラベルが独立（sigmoid）っぽく出ます。その場合、28列の合計が必ずしも 1 にならず、**“分布”というより“複数感情の強さ”**になります。

このタイプの値はモデルのベースレート（neutral が出やすい等）を受けやすく、平均化すると似通いやすいです。

### 原因D：標準化していないので、平均が大きい列や分散の小さい列が支配する

今は生の28次元をそのまま cosine に入れているため、

- 平均が大きい列（例：neutral）
- ほぼ定数に近い列（分散が極小）

がベクトルの「方向」を決めやすくなり、結果として全体が似た方向になります。

---

## 3. それって悪いの？（結論）

- **絶対値として 0.99 が多いこと自体は、必ずしもバグではありません。**
- ただし、スコアの絶対値が高すぎると **差が見えにくい**ため、推薦の納得感や多様性が落ちたり、Top-K が“何でも似てる”感じになりやすいです。
- 見るべきは「0.99だから似てる」よりも、**順位**と**上位の中での相対差**になりがちです。

---

## 4. 改善策（おすすめ順：軽いもの → 効くもの）

### 改善1：`neutral` を除外（または重みを下げる）

`neutral` が支配しているなら、まず外す（あるいは 0.2 倍などで弱める）のが手堅いです。

- 長所：実装が簡単、スコア分布が広がりやすい
- 短所：neutral の情報（淡々とした曲など）を捨てる/弱める

### 改善2：列ごとに標準化（z-score）してから cosine

各感情列について、全曲平均との差を見たいなら z-score が効きます。

- 手順：`x' = (x - mean) / std` を列ごとに適用 → `cosine(x', y')`
- 期待効果：ベースレートが支配しにくくなり、差が出やすい

注意：`std` が極端に小さい列は不安定なので、`std` の下限を設けるか、その列を除外するのが安全です。

### 改善3：全体平均ベクトルを引いてから cosine（mean-centered cosine）

「共通ベースが邪魔」問題に直撃する方法です。

- 手順：`μ = 全曲の平均ベクトル`、`x' = x - μ`、`cosine(x', y')`
- 期待効果：neutral を含めた共通成分をまとめて除去できる
- 欠点：負の値が出る（問題ではないが解釈は変わる）

### 改善4：pooling を工夫し、「感情が出ている行」を重くする

mean pooling で薄まるなら、薄まらないようにします。例：

- 重み = `(1 - neutral)` で重み付き平均（neutral が高い＝感情が薄い行の影響を下げる）
- 重み = 行ベクトルの L2 ノルム（感情が強い行を重視）
- neutral が高すぎる行を除外（例：neutral > 0.8 の行を捨てる）

長所：曲の“感情的な部分”が強調されて差が出やすい  
短所：パラメータ（閾値など）が増え、偏りも入り得る

### 改善5：距離尺度を変える（目的が“分布の違い”なら）

- Pearson 相関（= 平均中心化に近い）
- Jensen–Shannon divergence / Hellinger など

ただし「分布」前提の尺度は、（多ラベルで合計1でないなら）L1 正規化など前処理が必要で、意味付けが変わります。

---

## 5. 何からやるべき？（おすすめ）

「すぐ効いて説明もしやすい」順だと：

1. `neutral` 除外（または重みを下げる）
2. mean-centered cosine（`x - μ`）
3. z-score + cosine
4. 必要なら重み付き pooling（例：`1 - neutral`）

このどれかで、0.99 張り付きはかなり緩みやすいです。

---

## 6. 次の一手（実装案）

Streamlit サイドバーで「類似度の計算方法」を切り替えられるようにすると、挙動を比較できて便利です。例：

- raw cosine（現状）
- cosine without neutral
- mean-centered cosine
- zscore cosine
- （任意）weighted pooling（1-neutral）

